# Deep Learning Tutorials with Tensorflow
The deeplearning algorithms are carefully implemented by [tensorflow](https://www.tensorflow.org/).
### Environment
- Python 3.5
- tensorflow 0.12

### The deeplearning algorithms includes (now):
- Logistic Regression  [logisticRegression.py](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/models/logisticRegression.py)
- Multi-Layer Perceptron (MLP) [mlp.py](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/models/mlp.py)
- Convolution Neural Network (CNN) [cnn.py](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/models/cnn.py)
- Denoising Aotoencoder (DA) [da.py](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/models/da.py)
- Stacked Denoising Autoencoder (SDA) [sda.py](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/models/sda.py)
- Restricted Boltzmann Machine (RBM) [[rbm.py](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/models/rbm.py)    [gbrbm.py](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/models/gbrbm.py)]
- Deep Belief Network (DBN) [dbn.py](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/models/dbn.py)

Note: the project aims at imitating the well-implemented algorithms in [Deep Learning Tutorials](http://www.deeplearning.net/tutorial/) (coded by [Theano](http://deeplearning.net/software/theano/index.html)).

### CNN Models
- MobileNet [[self](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/CNNs/MobileNet.py) [paper](https://arxiv.org/abs/1704.04861) [ref](https://github.com/Zehaos/MobileNet/blob/master/nets/mobilenet.py)]
- SqueezeNet [[self](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/CNNs/SqueezeNet.py) [paper](https://arxiv.org/abs/1602.07360)]
- ResNet [[self](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/CNNs/ResNet50.py) [caffe ref](https://github.com/KaimingHe/deep-residual-networks) [paper1](https://arxiv.org/abs/1512.03385) [paper2](https://arxiv.org/abs/1603.05027)]



### Practical examples
You can find more practical examples with tensorflow here:
- CNN for setence classification [[self](https://github.com/xiaohu2015/DeepLearning_tutorials/tree/master/examples/cnn_setence_classification)] [[blog](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)] [[paper](https://arxiv.org/pdf/1408.5882v2.pdf)]
- RNN for language model [[self](https://github.com/xiaohu2015/DeepLearning_tutorials/tree/master/examples/rnn_language_model)] [[blog](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/)] [[blog_cn](http://blog.csdn.net/xiaohu2022/article/details/54578013)]
- LSTM for language model (PTB data) [[self](https://github.com/xiaohu2015/DeepLearning_tutorials/tree/master/examples/lstm_model_ptb)] [[tutorial](https://www.tensorflow.org/versions/r0.12/tutorials/recurrent/index.html#recurrent-neural-networks)] [[paper](https://arxiv.org/pdf/1409.2329.pdf)]
- VGG model for image classification (object recongnition) [[self](https://github.com/xiaohu2015/DeepLearning_tutorials/tree/master/examples/VGG)] [[source](https://github.com/machrisaa/tensorflow-vgg)]
- Residual network for cifar10_dataset [[self](https://github.com/xiaohu2015/DeepLearning_tutorials/tree/master/examples/Resnet)] [[source](https://github.com/wenxinxu/resnet-in-tensorflow)] [[paper](https://arxiv.org/pdf/1603.05027v3.pdf)]
- LSTM for time series prediction [[self](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/examples/lstm_time_series_regression)] [[source](https://github.com/MorvanZhou/tutorials/blob/master/tensorflowTUT/tf20_RNN2.2/full_code.py)]
- Generative adversarial network (GAN) [[self](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/examples/gan)]
- Variational autoencoder (VAE) [[self](https://github.com/xiaohu2015/DeepLearning_tutorials/tree/master/examples/VAE)]

### Results
![1](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/results/filters_corruption_30.png)
![2](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/results/new_filters_at_epoch_14.png)
![3](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/results/new_original_and_10samples.png)
![4](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/results/DBN_results.png)
![5](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/examples/lstm_time_series_regression/lstm_regression_results.png)

### Fun Blogs
- [Chatbots with Seq2Seq](http://suriyadeepan.github.io/2016-06-28-easy-seq2seq/)

### Personal Notes
- Tensorflow for RNNs [[tf_rnn.ipynb](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/notes/tf_rnn.ipynb)]
- Tensorflow for Autoencoder [[tf_autoencoder.ipynb](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/notes/tf_autoencoder.ipynb)]

### Other Tutorials
- [ageron/handson-ml
](https://github.com/ageron/handson-ml/)
- [Hvass-Labs/TensorFlow-Tutorials
](https://github.com/Hvass-Labs/TensorFlow-Tutorials)
- [BinRoot/TensorFlow-Book
](https://github.com/BinRoot/TensorFlow-Book)
- [sjchoi86/dl_tutorials_10weeks
](https://github.com/sjchoi86/dl_tutorials_10weeks)

what can AI do for you
# WHAT-AI-CAN-DO-FOR-YOU

“**_The business plans of the next 10,000 startups are easy to forecast: Take X and add AI._**” — Kevin Kelly

Here are the breakthrough AI papers and CODE for any industry.

[![Alt text](https://img.youtube.com/vi/edFHBAIAmEM/0.jpg)](https://www.youtube.com/watch?v=edFHBAIAmEM)

"*A hundred years ago electricity transformed countless industries; 20 years ago the internet did, too. Artificial intelligence is about to do the same. To take advantage, companies need to understand what AI can do.*" — Andrew Ng

If you are a newcomer to the AI, the first question you may have is "**_What AI can do now and how it relates to my strategies?_**" Here are the breakthrough AI papers and `CODE` for any industry.

## Deep Learning BOOKS

### 0.0 Deep Learning

**[0]** Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. ["**_Deep learning_**"](http://www.deeplearningbook.org) An MIT Press book. (2016).

"*Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.*" -- Elon Musk, co-chair of OpenAI; co-founder and CEO of Tesla and SpaceX

### 0.1 Deep Reinforcement Learning

**[1]** Richard S. Sutton and Andrew G. Barto. ["**_Reinforcement Learning: An Introduction (2nd Edition)_**"](https://webdocs.cs.ualberta.ca/%7Esutton/book/bookdraft2016sep.pdf)

**[2]** Pieter Abbeel and John Schulman | Open AI / Berkeley AI Research Lab. ["**_Deep Reinforcement Learning through
Policy Optimization_**"](http://people.eecs.berkeley.edu/~pabbeel/nips-tutorial-policy-optimization-Schulman-Abbeel.pdf)

**[3]** Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W. Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, Nando de Freitas. ["**_Learning to learn by gradient descent by gradient descent_**"](https://arxiv.org/pdf/1606.04474v2.pdf)

&nbsp;&nbsp;&nbsp;`CODE` [Learning to Learn in TensorFlow](https://github.com/deepmind/learning-to-learn)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; arXiv [Learning to Learn for Global Optimization of Black Box Functions](https://arxiv.org/pdf/1611.03824v3.pdf)

## Deep Learning PAPERS

### 1.0 Papers Reading Roadmap

**[0]** ["**_Deep Learning Papers Reading Roadmap_**"](https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap)

&nbsp;&nbsp;&nbsp;`CODE` [Download All Papers](https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap/blob/master/download.py)

![NIPS](NIPS-v0.png "NIPS")

### 1.1 Neural Information Processing Systems Conference - NIPS 2016

Dec 05–8, 2016
Centre Convencions Internacional Barcelona, Barcelona SPAIN

The Thirtieth Annual Conference on Neural Information Processing Systems (NIPS) is a multi-track machine learning and computational neuroscience conference that includes invited talks, demonstrations, symposia and oral and poster presentations of refereed papers. Following the conference, there are workshops which provide a less formal setting.

**[1]** Full Videos ["**_NIPS 2016 : 57 Episodes_**"](https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016)

**[2]** `CODE` ["**_All Code Implementations for NIPS 2016 papers_**"](https://www.reddit.com/r/MachineLearning/comments/5hwqeb/project_all_code_implementations_for_nips_2016/)

### 1.2 GitXiv : arXiv + Github + Links + Discussion

**[3]** arXiv + `CODE` ["**_Implementations of Some of the Best arXiv Papers_**"](http://www.gitxiv.com)

### 1.3 Wasserstein GAN

**[4]** arXiv ["**_Wasserstein GAN_**"](https://arxiv.org/pdf/1701.07875v1.pdf)

**[5]** `CODE` ["**_Code accompanying the paper "Wasserstein GAN"_**"](https://github.com/martinarjovsky/WassersteinGAN)

### 1.4 The Predictron

**[6]** arXiv ["**_The Predictron: End-To-End Learning and Planning_**"](https://arxiv.org/pdf/1612.08810v2.pdf)

**[7]** `CODE` ["**_A TensorFlow implementation of "The Predictron: End-To-End Learning and Planning"_**"](https://github.com/zhongwen/predictron)

### 1.5 Meta-RL

**[8]** arXiv ["**_Learning to reinforcement learn_**"](https://arxiv.org/pdf/1611.05763v3.pdf)

**[9]** `CODE` ["**_Meta-RL"_**"](https://github.com/awjuliani/Meta-RL)

### 1.6 Neural Architecture Search with RL

**[10]** arXiv ["**_Neural Architecture Search with Reinforcement Learning_**"](https://openreview.net/pdf?id=r1Ue8Hcxg)

### 1.7 Superior Generalizability and Interpretability

Recursion is the key to true generalisation.

**[11]** arXiv ["**_Making Neural Programming Architectures Generalize via Recursion_**"](https://openreview.net/pdf?id=BkbY4psgg)

### 1.8 Seq2seq RL GANs for Dialogue Generation

**[12]** arXiv ["**_Adversarial Learning for Neural Dialogue Generation_**"](https://arxiv.org/pdf/1701.06547.pdf)

### 1.9 DeepMind’s PathNet: Modular Deep Learning Architecture for AGI

For artificial general intelligence (AGI) it would be efficient if multiple users trained the same giant neural network, permitting parameter reuse, without catastrophic forgetting. PathNet is a first step in this direction. It is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to re-use for new tasks

**[13]** arXiv ["**_PathNet: Evolution Channels Gradient Descent in Super Neural Networks_**"](https://arxiv.org/pdf/1701.08734.pdf)

### 1.10 Outrageously Large Neural Networks

... achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.

**[14]** arXiv ["**_Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer_**"](https://arxiv.org/pdf/1701.06538.pdf)

## Deep Learning TUTORIALS

### 2.0 Implementation of Reinforcement Learning Algorithms

**[0]** `CODE` ["**_Implementation of Reinforcement Learning Algorithms. Python, OpenAI Gym, Tensorflow. Exercises and Solutions to accompany Sutton's Book and David Silver's course._**"](https://github.com/dennybritz/reinforcement-learning)

### 2.1 Python Data Science Handbook

**[1]** `CODE` ["**_Jupyter Notebooks for the Python Data Science Handbook_**"](https://github.com/jakevdp/PythonDataScienceHandbook) by Jake Vanderplas.

### 2.2 Learn How to Build State of the Art Models

**[2]** Video + `CODE` ["**_Practical Deep Learning For Coders, Part 1_**"](http://course.fast.ai) by Jeremy Howard.

### 2.3 NIPS 2016 Tutorial: Generative Adversarial Networks

**[3]** arXiv ["**_NIPS 2016 Tutorial: Generative Adversarial Networks_**"](https://arxiv.org/abs/1701.00160) by Ian Goodfellow.

### 2.4 Data Science IPython Notebooks

**[4]** `CODE` ["**_Data Science Python Notebooks: Deep learning (TensorFlow, Theano, Caffe), Scikit-learn, Kaggle, Big Data (Spark, Hadoop MapReduce, HDFS), Pandas, NumPy, SciPy..._**"](https://github.com/donnemartin/data-science-ipython-notebooks)

## Deep Learning TOOLS

![TensorFlow](TensorFlow-v0.png "TensorFlow")

### 3.0 TensorFlow

TensorFlow is an Open Source Software Library for Machine Intelligence: [https://www.tensorflow.org](https://www.tensorflow.org)

**[0]** Mart ́ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane ́, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vie ́gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. ["**_WhitePaper - TensorFlow: Large-scale machine learning on heterogeneous systems_**"](http://download.tensorflow.org/paper/whitepaper2015.pdf)

&nbsp;&nbsp;&nbsp;`CODE` [Installation](https://github.com/tensorflow/tensorflow)

&nbsp;&nbsp;&nbsp;`CODE` [TensorFlow Tutorial and Examples for Beginners](https://github.com/aymericdamien/TensorFlow-Examples)

&nbsp;&nbsp;&nbsp;`CODE` [Models built with TensorFlow](https://github.com/tensorflow/models)

### 3.1 OpenAI Gym

The OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms [https://gym.openai.com](https://gym.openai.com)

**[1]** Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba. ["**_OpenAI Gym WhitePaper_**"](https://arxiv.org/pdf/1606.01540v1.pdf)

&nbsp;&nbsp;&nbsp;`CODE` [Installation of the gym open-source library](https://github.com/openai/gym)

&nbsp;&nbsp;&nbsp;`CODE` [How to create new environments](https://github.com/openai/gym/tree/master/gym/envs#how-to-create-new-environments-for-gym)

### 3.2 Universe

Universe: A software platform for measuring and training an AI's general intelligence across the world's supply of games, websites and other applications. [Universe (blog)](https://openai.com/blog/universe/).

&nbsp;&nbsp;&nbsp;`CODE` [Installation](https://github.com/openai/universe)

&nbsp;&nbsp;&nbsp;`CODE` [Universe Starter Agent](https://github.com/openai/universe-starter-agent)

### 3.3 DyNet: The Dynamic Neural Network Toolkit

DyNet is a neural network library designed to be efficient when run on either CPU or GPU. DyNet has been used to build state-of-the-art systems for [syntactic parsing](https://github.com/clab/lstm-parser), [machine translation](https://github.com/neubig/lamtram), [morphological inflection](https://github.com/mfaruqui/morph-trans).

**[2]** Graham Neubig, Chris Dyer, Yoav Goldberg, Austin Matthews, Waleed Ammar, Antonios Anastasopoulos, Miguel Ballesteros, David Chiang, Daniel Clothiaux, Trevor Cohn, Kevin Duh, Manaal Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji, Lingpeng Kong, Adhiguna Kuncoro, Gaurav Kumar, Chaitanya Malaviya, Paul Michel, Yusuke Oda, Matthew Richardson, Naomi Saphra, Swabha Swayamdipta, Pengcheng Yin. ["**_DyNet: The Dynamic Neural Network Toolkit_**"](https://arxiv.org/pdf/1701.03980v1.pdf)

&nbsp;&nbsp;&nbsp;`CODE` [Installation](https://github.com/clab/dynet)

### 3.4 Edward: A Python library for Probabilistic Modeling, Inference and Criticism

[Edward](http://edwardlib.org) is a Python library for probabilistic modeling, inference and criticism fusing three fields: Bayesian statistics and machine learning, deep learning, and probabilistic programming. Runs on TensorFlow.

**[3]** Dustin Tran, Matthew D. Hoffman, Rif A. Saurous, Eugene Brevdo, Kevin Murphy, David M. Blei. ["**_Deep Probabilistic Programming_**"](https://arxiv.org/pdf/1701.03757v1.pdf)

&nbsp;&nbsp;&nbsp;`CODE` [Installation](https://github.com/blei-lab/edward)

### 3.5 DeepMind Lab: A customisable 3D platform for agent-based AI research

DeepMind Lab provides a suite of challenging 3D navigation and puzzle-solving tasks for learning agents. Its primary purpose is to act as a testbed for research in artificial intelligence, especially deep reinforcement learning.

**[4]** Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg, Stig Petersen. ["**_DeepMind Lab_**"](https://arxiv.org/pdf/1612.03801v2.pdf)

&nbsp;&nbsp;&nbsp;`CODE` [Installation of the DeepMind Lab](https://github.com/deepmind/lab)

## Breakthrough AI Papers and CODE for Any Industry - WORK IN PROGRESS

The following is constructed in accordance with the following three guiding principles:

1. *Focus on state-of-the-art*;
2. *From generic to specific areas*; and
3. *Clarity, efficiency and transparency*.

Being able to deploy with the least possible delay is key.

| Industry        | What AI           | Papers  | `CODE`  |
| -------------------------- |:--------------------------:|:--------------------------:|:-----:|
| Robotics      | Deep Reinforcement Learning | ["**_Extending the OpenAI Gym for robotics_**"](https://arxiv.org/pdf/1608.05742v1.pdf) | ["**_Gym Gazebo_**"](https://github.com/erlerobot/gym-gazebo/)
| Translation      | Multilingual Neural Machine Translation (NMT)      | ["**_Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation_**"](https://arxiv.org/pdf/1611.04558v1.pdf)   | ["**_OpenNMT_**"](https://github.com/OpenNMT/OpenNMT)
| Word Embeddings      | Approximate Factorization of the Point-Wise Mutual Information Matrix via Stochastic Gradient Descent      | ["**_Swivel: Improving Embeddings by Noticing What's Missing_**"](https://arxiv.org/pdf/1602.02215v1.pdf) | ["**_Swivel_**"](https://github.com/tensorflow/models/tree/master/swivel)
| Chemistry and Drug Discovery      | Deep Neural Network and Monte Carlo Tree Search (MCTS)     | ["**_Towards "AlphaChem": Chemical Synthesis Planning with Tree Search and Deep Neural Network Policies_**"](https://arxiv.org/pdf/1702.00020v1.pdf)   | ["**_DeepChem_**"](https://github.com/deepchem/deepchem)
| Art      |       |    |
| NLP(Natural Language Processing) |       |     |
| Audio | Deep Neural Network      | ["**_WaveNet: A Generative Model for Raw Audio_**"](https://arxiv.org/pdf/1609.03499v2.pdf)    | ["**_Tensorflow-Wavenet_**"](https://github.com/ibab/tensorflow-wavenet)
| Image Caption |       |     |
| Image Recognition | Very Deep Convolutional Networks      | ["**_Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning_**"](https://arxiv.org/pdf/1602.07261v2.pdf)    | ["**_Keras-InceptionV4n_**"](https://github.com/kentsommer/keras-inceptionV4)
| Full Resolution Image Compression  | Recurrent Neural Networks      | ["**_Full Resolution Image Compression with Recurrent Neural Networks_**"](https://arxiv.org/pdf/1608.05148v1.pdf)    | ["**_Compression_**"](https://github.com/tensorflow/models/tree/master/compression)
| Visual Tracking |       |     |
| No-Limit Poker | Blend of Deep Learning and Classical AI | DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker [[arXiv](https://arxiv.org/abs/1701.01724)] |
| Recommender Systems |       |     |
| Bioinformatics |       |     |
| Neural Network Chip |       |     |
| Game |       |     |
